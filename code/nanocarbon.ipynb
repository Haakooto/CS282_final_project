{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import utils\n",
    "from BayesModel import BFC, FC\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 22}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_pred(model, Y_test, X_test, enums=10, single=False):\n",
    "    Y_sample = Y_test.detach().numpy()\n",
    "\n",
    "    # sample test data enums times and average to make a prediction\n",
    "    Y_pred = torch.zeros(enums, *Y_test.size())\n",
    "    for j in range(enums):\n",
    "        Y_pred[j] = model(X_test)\n",
    "\n",
    "    if single:  # Plots all the predictions of a single data point\n",
    "        y1 = Y_pred.detach().numpy()[:, 1]\n",
    "        Y = Y_sample[1]\n",
    "        pred_mean = y1.mean()\n",
    "        x = np.linspace(0, 1, len(y1))\n",
    "        plt.plot(x, y1, \"bo\", label=\"all preds for single datapoint\")\n",
    "        plt.plot(x, np.ones_like(x) * Y_sample[1], \"r--\", lw=5, label=\"true value\")\n",
    "        plt.plot(x, np.ones_like(x) * pred_mean, \"k--\", lw=4, label=\"prediction mean\")\n",
    "        plt.plot(x, pred_mean + np.ones_like(x) * y1.std(), \"g--\", lw=4, label=\"prediction std\")\n",
    "        plt.plot(x, pred_mean - np.ones_like(x) * y1.std(), \"g--\", lw=4)\n",
    "        plt.xlabel(\"Prediction count\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.title(\"All predictions for a single datapoint after training\")\n",
    "    else:  # Plot mean of all predictions for 40 datapoints, with error bounds\n",
    "        y1 = Y_pred.detach().numpy()\n",
    "        pred_mean = y1.mean(0) \n",
    "        pred_std = y1.std(0)\n",
    "        print(\"mean mean: \", pred_mean.mean())\n",
    "        print(\"mean std:\", pred_mean.std())\n",
    "        print(\"std mean: \", pred_std.mean())\n",
    "        print(\"std std:\", pred_std.std())\n",
    "        x = np.arange(len(Y_sample))\n",
    "        xx = np.c_[x, x, x, x, x, x] if Y_test.size(1) == 3 else np.c_[x, x]\n",
    "        interval = np.c_[pred_mean - pred_std, pred_mean + pred_std]\n",
    "        R2 = 1 - ((Y_sample - pred_mean) ** 2).sum() / ((Y_sample - Y_sample.mean()) ** 2).sum()\n",
    "    \n",
    "        plt.plot(Y_sample[:40], \"bo\", ms=8, label=\"target\")\n",
    "        plt.plot(pred_mean[:40], \"ro\", ms=4, label=\"mean\")\n",
    "        plt.plot(xx[:40], interval[:40], \"go\", ms=4, label=\"1 sigma\")\n",
    "        plt.title(f\"R2: {R2}\")\n",
    "        plt.xlabel(\"Datapoints\")\n",
    "        plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_pred_freq(model, Y_test, X_test):\n",
    "    Y_sample = Y_test.detach().numpy()\n",
    "    Y_pred_sample = model(X_test.to(torch.float)).detach().numpy()\n",
    "\n",
    "    R2 = 1 - ((Y_sample - Y_pred_sample) ** 2).sum() / ((Y_sample - Y_sample.mean()) ** 2).sum()\n",
    "    plt.plot(Y_sample[:100], \"bo-\", ms=8, label=\"target\")\n",
    "    plt.plot(Y_pred_sample[:100], \"ro--\", ms=4, label=\"prediction\")\n",
    "    plt.title(f\"R2-score on test data: {R2}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(*, model, optimer, data, device, epochs, enums=10, kl_on = True):\n",
    "    pbar = tqdm(range(epochs))\n",
    "    losses = {\"total\": [], \"recon\": [], \"kl\": []}\n",
    "    for epoch in pbar:\n",
    "        for batch_idx, (x, y) in enumerate(data):\n",
    "            optimer.zero_grad()\n",
    "                # sample batch enums times during training and average prediction before calculating loss\n",
    "            outs = torch.zeros(enums, *y.size())\n",
    "            for j in range(enums):\n",
    "                outs[j] = model(x, train=True)\n",
    "            \n",
    "            pred = outs.mean(0)  # take average of all predictions of each datapoint as final prediction of that datapoint \n",
    "            loss_recon = -torch.distributions.Normal(pred, 0.1).log_prob(y).mean()\n",
    "\n",
    "            if kl_on:\n",
    "                loss_kl = model.kl_reset() / (len(x) * enums)  # normalise by number of batches and enums\n",
    "                loss =  loss_recon + loss_kl\n",
    "            else:\n",
    "                loss =  loss_recon\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimer.step()\n",
    "\n",
    "        if kl_on:    \n",
    "            pbar.set_description(f\"total loss: {loss:.4f}, recon. loss: {loss_recon:.4f}, kl_loss: {loss_kl:.4f}\")\n",
    "            losses[\"kl\"].append(loss_kl.detach().numpy())\n",
    "        else:\n",
    "            pbar.set_description(f\"total loss: {loss:.4f}, recon. loss: {loss_recon:.4f}\")\n",
    "        losses[\"total\"].append(loss.detach().numpy())\n",
    "        losses[\"recon\"].append(loss_recon.detach().numpy())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def train_model_freq(*, model, optimer, data, device, epochs):\n",
    "    losses = []\n",
    "    pbar = tqdm(range(epochs))\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    for epoch in pbar:\n",
    "        for bi, (x, y) in enumerate(data):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimer.zero_grad()\n",
    "\n",
    "            # was simpler to copy code than generalize for non-kl_divergence\n",
    "            pred = model(x.to(torch.float))\n",
    "            loss = loss_fn(pred, y.to(torch.float))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimer.step()\n",
    "\n",
    "        losses.append(loss.detach().numpy())\n",
    "        pbar.set_description(f\"total loss: {loss:.4f}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses, model=\"FNN\", save=False, show=False):\n",
    "    plt.plot(losses[\"total\"][0:], label=\"total loss\")\n",
    "    plt.plot(losses[\"recon\"][0:], label=\"recon loss\")\n",
    "    plt.plot(losses[\"kl\"][0:], label=\"kl loss\")\n",
    "    title = f'{model} Losses'\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    if show: plt.show()\n",
    "    if save: plt.savefig(f'figures/carbon_{title.replace(\" \", \"_\")}', bbox_inches='tight', dpi=300)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_uncertainty(x, y, gnet, train_size, net_type, enums=500, save=False, show=False):\n",
    "    y_pred = torch.zeros(enums, *y.size())\n",
    "\n",
    "    for i in tqdm(range(enums)):\n",
    "        y_pred[i] = gnet(x)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    y_mean = y_pred.mean(axis=0)\n",
    "    y_sigma = y_pred.std(axis=0)\n",
    "    \n",
    "    a = 50\n",
    "    aaaa = np.arange(a)\n",
    "\n",
    "    R2 = 1 - ((y - y_mean) ** 2).sum() / ((y - y.mean()) ** 2).sum()\n",
    "    plt.plot(aaaa, y_mean[:a], 'ro', lw=1, label='Predictive mean')\n",
    "    plt.plot(aaaa, y[:a], \"bo\", ms=4, lw=1, label='Target value')\n",
    "    plt.fill_between(aaaa.ravel(), \n",
    "                    (y_mean + 2 * y_sigma)[:a, 0], \n",
    "                    (y_mean - 2 * y_sigma)[:a, 0], \n",
    "                    alpha=0.5, label='Epistemic uncertainty')\n",
    "    plt.title(f'Prediction with R2-score on test data: {R2:.4f}')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel(\"data-points\")\n",
    "    plt.ylabel(\"coordinate value\")\n",
    "    # plt.legend()\n",
    "    if show: plt.show()\n",
    "    if save:\n",
    "        filepath = f'figures/carbon_{net_type}_{train_size}'.replace(\".\", \"\")\n",
    "        plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    plt.clf()\n",
    "\n",
    "def plot_pred_freq(x, y, fnet, train_size, params, save=False, show=False):\n",
    "    y_pred = fnet(x.to(torch.float)).detach().numpy()\n",
    "\n",
    "    R2 = 1 - ((y - y_pred) ** 2).sum() / ((y - y.mean()) ** 2).sum()\n",
    "\n",
    "    plt.plot(y_pred[:50], 'ro', lw=1, label='Predicted value')\n",
    "    plt.plot(y[:50], \"bo\", ms=4, lw=1, label='Target value')\n",
    "    plt.title(f'Prediction with R2-score on test data: {R2:.4f}')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"data-points\")\n",
    "    plt.ylabel(\"coordinate value\")\n",
    "    if show: plt.show()\n",
    "    if save:\n",
    "        filepath = f'figures/carbon_FCNET_ts{train_size}_l{len(params[0])}'.replace(\".\", \"\")\n",
    "        plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAUSS 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecce523273c42378fa9277674f80391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3fbb07bceb45abac418686ca568d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAUSS 0.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180885e10ee540028819ed9bd60a9c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3810be5337b8499ba46ce4ff27ed584f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_params = [(0.2, 200), (0.99, 10)]\n",
    "fc_params = [([20, 20], 1000), ([50, 50, 50, 50, 50, 50], 5000)]\n",
    "\n",
    "for test_s, batch_s in data_params:\n",
    "    train_loader, test_data = utils.get_nanotube_data(test_size=test_s, batch_size=batch_s)\n",
    "    X_test, Y_test = test_data\n",
    "\n",
    "    # for params in fc_params:\n",
    "    #     print(\"FC\", test_s, params)\n",
    "    #     fnet = FC(features=7, classes=1, hiddens=params[0])\n",
    "    #     foptimizer = torch.optim.AdamW(fnet.parameters(), lr=0.001)\n",
    "    #     losses = train_model_freq(model=fnet, optimer=foptimizer, data=train_loader, device=device, epochs=15)\n",
    "    #     plot_pred_freq(X_test, Y_test, fnet, test_s, params, save=True, show=False)\n",
    "    \n",
    "    # print(\"VMF\", test_s)\n",
    "    # vnet = BFC(features=7, classes=1, hiddens=[20,20], prior={\"dist\": \"vmf\", \"loc\": 1, \"scale\": .1, \"record\": False, \"dist_kwargs\": {\"k\": 100}})\n",
    "    # voptimizer = torch.optim.AdamW(vnet.parameters(), lr=.001)\n",
    "    # losses = train_model(model=vnet, optimer=voptimizer, data=train_loader, device=device, epochs=15, enums=10, kl_on=True)\n",
    "    # plot_pred_uncertainty(X_test, Y_test, vnet, test_s, 'VmfNET_highlr', enums=500, save=True, show=False)\n",
    "        \n",
    "    print('GAUSS', test_s)\n",
    "    gnet = BFC(features=7, classes=1, hiddens=[20, 20], prior={\"dist\": \"normal\", \"loc\": 0, \"scale\": .1, \"record\": False})\n",
    "    goptimizer = torch.optim.AdamW(gnet.parameters(), lr=0.001)\n",
    "    losses = train_model(model=gnet, optimer=goptimizer, data=train_loader, device=device, epochs=15, enums=10, kl_on=True)\n",
    "    plot_pred_uncertainty(X_test, Y_test, gnet, test_s, 'GaussNET_redo', enums=500, save=True, show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e8d3d4799f327da20dd18e7e7a5bd6c8a402c99124f53a7c81166451855a22d"
  },
  "kernelspec": {
   "display_name": "3.9.2",
   "language": "python",
   "name": "python-3.9.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
